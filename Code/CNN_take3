import matplotlib as plt
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.utils.data import Dataset
from torchvision.transforms import ToTensor
import pandas as pd
import numpy as np
import csv
import glob
import scipy.io as sio
import os
import torch.nn.functional as F
import tqdm
from torch.utils.tensorboard import SummaryWriter
def main():
    class TrainingDataSet(Dataset):
        def __init__(self, path):
            self.data_path = path
            # self.files = glob.glob(self.path)
            # # self.files = glob.glob(r'C:\Users\Jakob\Desktop\AAU\7Semester\Projekt\P7_projekt\DATA\OFF_3\*.mat')
            # self.data = []
            # for datapath in self.files:
            #     self.data.append(datapath)
            # self.data_path = os.path.join(self.data[fileidx])
            self.dataset = sio.loadmat(self.data_path)
            
        def __getitem__(self, idx):
            self.label = self.dataset['ic_clean']['trialinfo'][0][0][idx]
            temp = []
            for i in self.label[18:]:
                temp.append(chr(i))
            temp = "".join(temp)
            if temp.find('Male') < temp.find('Female'):
                self.label = torch.zeros(1)
            else:
                self.label = torch.ones(1)
            self.currdataset = self.dataset['ic_clean']['trial'][0][0][0][idx]
            self.currdataset = self.currdataset[:-2, 4096:]
            return self.currdataset, self.label
        
        def __len__(self):
            return 20
        
    class SplitTrainingDataSet(Dataset):
        def __init__(self, path, fileidx):
            self.data_path = path
            self.data = sio.loadmat(self.data_path)
            self.label = self.data['ic_clean']['trialinfo'][0][0][fileidx]
            temp = []
            for i in self.label[18:]:
                temp.append(chr(i))
            temp = "".join(temp)
            if temp.find('Male') < temp.find('Female'):
                self.label = torch.tensor([1, 0])
            else:
                self.label = torch.tensor([0, 1])
            
            self.currdataset = self.data['ic_clean']['trial'][0][0][0][fileidx]
            self.currdataset = self.currdataset[:-2, 4096:]
                

        def __getitem__(self, idx):
            if idx == 0:
                self.batch = self.currdataset[:, 0:512]
            else:
                self.batch = self.currdataset[:, 512*idx:512*idx+512]
            return self.batch, self.label
    
        def __len__(self):
            return 20


    class ValidationDataSet(Dataset):
        def __init__(self, path):
            self.data_path = path
            # self.files = glob.glob(self.path)
            # self.files = glob.glob(r'C:\Users\Jakob\Desktop\AAU\7Semester\Projekt\P7_projekt\DATA\OFF_3\*.mat')
            # self.data = []
            # for datapath in self.files:
            #     self.data.append(datapath)
            # self.data_path = os.path.join(self.data[fileidx])
            self.dataset = sio.loadmat(self.data_path) 
        def __getitem__(self, idx):
            self.label = self.dataset['ic_clean']['trialinfo'][0][0][idx]
            temp = []
            for i in self.label[18:]:
                temp.append(chr(i))
            temp = "".join(temp)
            if temp.find('Male') < temp.find('Female'):
                self.label = torch.zeros(1)
            else:
                self.label = torch.ones(1)
            self.currdataset = self.dataset['ic_clean']['trial'][0][0][0][idx]
            self.currdataset = self.currdataset[:-2, 4096:]
            return self.currdataset, self.label
        
        def __len__(self):
            return 20
        

    class SplitValData(Dataset):
        def __init__(self, path, fileidx):
            self.data_path = path
            self.data = sio.loadmat(self.data_path)
            self.label = self.data['ic_clean']['trialinfo'][0][0][fileidx]
            temp = []
            for i in self.label[18:]:
                temp.append(chr(i))
            temp = "".join(temp)
            if temp.find('Male') < temp.find('Female'):
                self.label = torch.tensor([1, 0])
            else:
                self.label = torch.tensor([0, 1])
            self.currdataset = self.data['ic_clean']['trial'][0][0][0][fileidx]
            self.currdataset = self.currdataset[:-2, 4096:]
        def __len__(self):
            return 20
        def __getitem__(self, idx):
            if idx == 0:
                self.batch = self.currdataset[:, 0:512]
            else:
                self.batch = self.currdataset[:, 512*idx:512*idx+512]
            return self.batch, self.label
    class CNN(nn.Module):
        def __init__(self):
            super().__init__()
            self.conv1 = nn.Sequential(
            nn.Conv2d(1, 64, (1, 256), 1, padding='same'),
            nn.BatchNorm1d(64),
            nn.ELU()
            )
            self.conv2 = nn.Sequential(
                # nn.BatchNorm1d(64),
                nn.Conv2d(64, 64, (1, 32), 1, padding='same'),
                nn.ELU()
            )
            # 
            self.conv3 = nn.Sequential(
                nn.Conv2d(64, 64, (64, 1), 1, padding='valid'),
                nn.AvgPool2d(1,4),
                nn.Dropout(0.5),
                nn.ELU()
            )
            self.conv4 = nn.Sequential(
                # nn.BatchNorm1d(64),
                nn.Conv2d(64, 64, (1, 32), 1, padding='same'),
                nn.Conv2d(64, 64, (1, 1), 1, padding='same'),
                nn.AvgPool2d(1,8),
                nn.ELU()
            )
            self.fc1 = nn.Sequential(
                nn.Linear(1024, 2),
                nn.ELU(),
            )
            # self.fc2 = nn.Sequential(
            #     nn.Linear(2000, 500),
            #     nn.ELU()
            # )
            # self.fc3 = nn.Sequential(
            #     nn.Linear(500, 2),
            #     nn.ELU()
            # )
            # self.fc4 = nn.Sequential(
            #     nn.Linear(800, 1),
            #     nn.PReLU()
            # )
        def forward(self, x):
            x = self.conv1(x)
            x = self.conv2(x)
            x = self.conv3(x)
            x = self.conv4(x)
            x = torch.flatten(x)
            x = self.fc1(x)
            # x = self.fc2(x)
            # x = self.fc3(x)
            # x = self.fc4(x)
            # x = F.softmax(x)
            return x
    # conv1 = nn.Sequential(
    #         nn.Conv2d(1, 64, (1, 256), 1, padding = 'same'),
    #         nn.BatchNorm2d(64),
    #         nn.ELU()
    #         )
    # conv2 = nn.Sequential(
    #     nn.Conv2d(64, 64, (64, 1), 1, padding='valid'),
    #     nn.AvgPool2d(1,4),
    #     nn.ELU()
    # )
    # conv3 = nn.Sequential(
    #     nn.Conv2d(64, 64, (1, 16), 1, padding = 'same'),
    #     # nn.MaxPool2d(4,4),
    #     nn.Conv2d(64, 64, (1, 1), 1, padding='same'),
    #     nn.AvgPool2d(1,8),
    #     nn.ELU()
    # )
    # fc1 = nn.Sequential(
    #     nn.Linear(20928 , 5232),
    #     nn.ReLU()
    # )


    # path = r"C:\Users\Jakob\Desktop\AAU\7Semester\Projekt\P7_projekt\DATA\ON_3\ID0001_preprocesseddata_3dBON.mat"
    # training_data = SplitTrainingDataSet(path, 0)

    # train_dataloader = DataLoader(training_data, batch_size=2, shuffle=False)

    # testasd, labels = next(iter(train_dataloader))
    # print(testasd.shape)

    # asd = torch.unsqueeze(testasd, 1)
    # print(asd.shape)
    # test1 = conv1(asd.float())
    # print(test1.shape)
    # test2 = conv2(test1) 
    # print(test2.shape)
    # test3 = conv3(test2)
    # print(test3.shape)
    # # test3 = conv3(test2)
    # # test4 = torch.flatten(test3)
    # print(test2.shape)
    # # print(test3.shape)
    # # print(asd.shape)
    # print(test4.shape)
    # print(asd[0, :, 0, 0])


    # training_data = SplitTrainingDataSet(path, 0)
    # train_dataloader = DataLoader(training_data, batch_size=10, shuffle=False)
    # test_dataloader = DataLoader(validation_data, batch_size = 1, shuffle = False)



    # device = 'cuda' if torch.cuda.is_available() else 'cpu'
    # model = CNN().to(device)


    # model.load_state_dict(torch.load(r"C:\Users\Jakob\Desktop\AAU\7Semester\Projekt\P7_projekt\ModelDict.tar"))


    # loss_fn = nn.CrossEntropyLoss()
    # # writer = SummaryWriter()
    # optimizer = torch.optim.Adam(model.parameters(), lr= 0.0001)
    # paths = [
    #     r'C:\Users\Jakob\Desktop\AAU\7Semester\Projekt\P7_projekt\DATA\ON_3\*.mat', 
    #     r'C:\Users\Jakob\Desktop\AAU\7Semester\Projekt\P7_projekt\DATA\ON_8\*.mat', 
    #     r'C:\Users\Jakob\Desktop\AAU\7Semester\Projekt\P7_projekt\DATA\OFF_3\*.mat',
    #     r'C:\Users\Jakob\Desktop\AAU\7Semester\Projekt\P7_projekt\DATA\OFF_8\*.mat'
    # ]

    # # testing_data_path = r"C:\Users\Jakob\Desktop\AAU\7Semester\Projekt\P7_projekt\DATA\Testing"

    # for path in paths:
    #     files = glob.glob(path)
    #     for file in files:
    #         if file == r'C:\Users\Jakob\Desktop\AAU\7Semester\Projekt\P7_projekt\DATA\ON_8\*.mat':
    #             maxfileidx = 19
    #         else:
    #             maxfileidx = 20
    #         for e in range(0, maxfileidx):    
    #                 model.train()
    #                 training_data = SplitTrainingDataSet(file, e)
    #                 train_dataloader = DataLoader(training_data, batch_size=25, shuffle=True)
    #                 iteration = iter(train_dataloader)
    #                 data, labels = next(iteration)
    #                 # print(data.shape)
    #                 # with tqdm.tqdm(train_dataloader, unit = 'batch') as tepoch:
    #                 for i, x in enumerate(data):
    #                     # tepoch.set_description(f"Epoch {e}")
    #                     x = torch.unsqueeze(x, 0)
    #                     test = x.float().to(device)
    #                     # print(x.shape)
    #                     # y = torch.squeeze(labels[0], 1).to(device)
    #                     y = labels[i].float().to(device)
    #                     y_pred = model(test)

    #                     loss = loss_fn(y_pred, y)
    #                     # writer.add_scalar("Loss/train", loss, e)
    #                     optimizer.zero_grad()
    #                     loss.backward()
    #                     optimizer.step()
    #                     # acc += (y_pred.round() == y).float().sum()
    #                     # tepoch.set_postfix(loss=loss.item(), accuracy= acc)
    #                     print(f"\nLABEL: {y} PRED{y_pred}:") 
    # #             # writer.flush()


    # torch.save(model.state_dict(), r"C:\Users\Jakob\Desktop\AAU\7Semester\Projekt\P7_projekt\ModelDict.tar")


    # val_path = r'C:\Users\Jakob\Desktop\AAU\7Semester\Projekt\P7_projekt\DATA\Validation\*.mat'
    # files = glob.glob(val_path)
    # correct = 0
    # total = 0
    # for path in files:
    #         for i in range(20):
    #             validation_data = SplitValData(path, i)
    #             # for i in range(0,20):
    #             val_dataloader = DataLoader(validation_data, batch_size = 1, shuffle = False)
    #             # model.eval()     # Optional when not using Model Specific layer
    #             for data, labels in val_dataloader:
    #                 if torch.cuda.is_available():
    #                     data, labels = data.cuda().float(), torch.squeeze(labels, 1).cuda()
    #                 print(labels)
    #                 target = model(data)
    #                 print(target)
    #                 pred = torch.argmax(target)
    #                 print(pred)

    #                 if pred == torch.argmax(labels):
    #                     correct +=1
    #                 print(f'PREDICTION is {target}, LABEL IS {labels}')
    #                 # print(correct)
    #                 total += 1
    #                 # loss = loss_fn(target,labels)
    #                 # valid_loss = loss.item() * data.size(0)
    #                 # print(len(val_dataloader))

    # print(total, correct)
    # #     # # print(f'Epoch {i+1} \t\t  \t\t Validation Loss: {valid_loss / len(val_dataloader)}')
if __name__ == '__main__':
    main()