import matplotlib.pyplot as plt
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.utils.data import Dataset
from torchvision.transforms import ToTensor
from torchsummary import summary
import pandas as pd
import numpy as np
import csv
import glob
import scipy.io as sio
import os
import torch.nn.functional as F
import tqdm
from torch.utils.tensorboard import SummaryWriter
def main(): 
    #Dataset Classes for Dataloading
    class SplitTrainingDataSet(Dataset):
        def __init__(self, path):
            self.data_path = path
            self.data = sio.loadmat(self.data_path)
            self.labels = []
            for i in range(len(self.data['ic_clean']['trialinfo'][0][0])):
                self.label = self.data['ic_clean']['trialinfo'][0][0][i]
                temp = []
                for k in self.label[18:]:
                    temp.append(chr(k))
                temp = "".join(temp)
                if temp.find('Male') < temp.find('Female'):
                    self.labels.append(1)
                else:
                    self.labels.append(0)
            self.labels = np.repeat(self.labels, 33)
            self.dataset = []
            for j in self.data['ic_clean']['trial'][0][0][0]:
                self.dataset.append(j[:-2, 4096:])
            self.dataset = np.hstack(self.dataset)
            

        def __getitem__(self, idx):
            self.templabel = self.labels[idx]
            if self.templabel == 0:
                self.label = torch.tensor([0, 1])
            else:
                self.label = torch.tensor([1, 0])
            
            self.batch = self.dataset[:, 512*idx:512*idx+512]
            return self.batch, self.label
    
        def __len__(self):
            return len(self.dataset[0])//512

    class SplitValData(Dataset):
        def __init__(self, path, fileidx):
            self.data_path = path
            self.data = sio.loadmat(self.data_path)
            self.label = self.data['ic_clean']['trialinfo'][0][0][fileidx]
            temp = []
            for i in self.label[18:]:
                temp.append(chr(i))
            temp = "".join(temp)
            if temp.find('Male') < temp.find('Female'):
                self.label = torch.tensor([1, 0])
            else:
                self.label = torch.tensor([0, 1])
            self.currdataset = self.data['ic_clean']['trial'][0][0][0][fileidx]
            self.currdataset = self.currdataset[:-2, 4096:]
        def __len__(self):
            return 33
        def __getitem__(self, idx):
            self.batch = self.currdataset[:, 512*idx:512*idx+512]
            return self.batch, self.label
        
    # CNN Model
    class EEGNet(nn.Module):
        def __init__(self):
            super(EEGNet, self).__init__()

            self.F1 = 16
            self.F2 = 32    
            self.D = 2
            
            # Conv2d(in,out,kernel,stride,padding,bias)
            self.conv1 = nn.Sequential(
                nn.Conv2d(1, self.F1, (1, 256), padding=(0, 32), bias=False),
                nn.BatchNorm2d(self.F1)
            )
            
            self.conv2 = nn.Sequential(
                nn.Conv2d(self.F1, self.D*self.F1, (64, 1), groups=self.F1, bias=False),
                nn.BatchNorm2d(self.D*self.F1),
                nn.PReLU(),
                nn.AvgPool2d((1, 4)),
                nn.Dropout(0.5)
            )
            
            self.Conv3 = nn.Sequential(
                nn.Conv2d(self.D*self.F1, self.D*self.F1, (1, 16), padding=(0, 8), groups=self.D*self.F1, bias=False),
                nn.Conv2d(self.D*self.F1, self.F2, (1, 1), bias=False),
                nn.BatchNorm2d(self.F2),
                nn.PReLU(),
                nn.AvgPool2d((1, 8)),
                nn.Dropout(0.5)
            )
            
            self.classifier = nn.Linear(320, 2, bias=True)
            
        def forward(self, x):
            
            x = self.conv1(x)
            x = self.conv2(x)
            x = self.Conv3(x)
            
            x = torch.flatten(x)
            x = self.classifier(x)
            return x    
#############################################################################################################################
    # data_path = r"C:\Users\Jakob\Desktop\AAU\7Semester\Projekt\P7_projekt\DATA\ON_8\ID0001_preprocesseddata_8dBON"
    # data = sio.loadmat(data_path)
    # labels = []
    # for i in range(len(data['ic_clean']['trialinfo'][0][0])):
    #     label = data['ic_clean']['trialinfo'][0][0][i]
    #     temp = []
    #     for i in label[18:]:
    #         temp.append(chr(i))
    #     temp = "".join(temp)
    #     if temp.find('Male') < temp.find('Female'):
    #         labels.append(1)
    #     else:
    #         labels.append(0)
    # print(labels)
    # # labels = np.repeat(labels, 33)

    # # print(labels)
    # dataset = []
    # for j in data['ic_clean']['trial'][0][0][0]:
    #     dataset.append(j[:-2, 4096:])
    # test = np.hstack(dataset)


    # # # asd = test[0:64, 0:1536]
    # # # asd2 = test[65*1, 1536*1+1536]

    # print(len(test[0]))
    # print(len(test))
    # conv1 = nn.Sequential(
    #         nn.Conv2d(1, 64, (1, 256), 1, padding = 'same'),
    #         nn.BatchNorm2d(64),
    #         nn.PReLU()
    #         )
    # conv2 = nn.Sequential(
    #     nn.Conv2d(64, 64, (64, 1), 1, padding='valid'),
    #     nn.AvgPool2d(1,4),
    #     nn.ELU()
    # )
    # conv3 = nn.Sequential(
    #     nn.Conv2d(64, 64, (1, 16), 1, padding = 'same'),
    #     # nn.MaxPool2d(4,4),
    #     nn.Conv2d(64, 64, (1, 1), 1, padding='same'),
    #     nn.AvgPool2d(1,8),
    #     nn.ELU()
    # )
    # fc1 = nn.Sequential(
    #     nn.Linear(20928 , 5232),
    #     nn.ReLU()
    # )
                # Conv2d(in,out,kernel,stride,padding,bias)
    # D = 2
    # F1 = 16
    # F2 = 8
    # conv1 = nn.Sequential(
    #     nn.Conv2d(1, F1, (1, 64), padding=(0, 32), bias=False),
    #     nn.BatchNorm2d(F1)
    # )
    
    # conv2 = nn.Sequential(
    #     nn.Conv2d(F1, D*F1, (22, 1), groups=F1, bias=False),
    #     nn.BatchNorm2d(D*F1),
    #     nn.ELU(),
    #     nn.AvgPool2d((1, 4)),
    #     nn.Dropout(0.5)
    # )
    
    # conv3 = nn.Sequential(
    #     nn.Conv2d(D*F1, D*F1, (1, 16), padding=(0, 8), groups=D*F1, bias=False),
    #     nn.Conv2d(D*F1, F2, (1, 1), bias=False),
    #     nn.BatchNorm2d(F2),
    #     nn.ELU(),
    #     nn.AvgPool2d((1, 8)),
    #     nn.Dropout(0.5)
    # )

    # path = r"C:\Users\Jakob\Desktop\AAU\7Semester\Projekt\P7_projekt\DATA\ON_3\ID0001_preprocesseddata_3dBON.mat"
    # training_data = SplitTrainingDataSet(path, 0)

    # train_dataloader = DataLoader(training_data, batch_size=2, shuffle=False)

    # testasd, labels = next(iter(train_dataloader))
    # print(testasd.shape)

    # asd = torch.unsqueeze(testasd, 1)
    # print(asd.shape)
    # test1 = conv1(asd.float())
    # print(test1.shape)
    # test2 = conv2(test1) 
    # print(test2.shape)
    # test3 = conv3(test2)
    # print(test3.shape)
    # test4 = torch.flatten(test3)
    # print(test4.shape)
    # # test3 = conv3(test2)
    # # test4 = torch.flatten(test3)
    # print(test2.shape)
    # # print(test3.shape)
    # # print(asd.shape)
    # print(test4.shape)
    # print(asd[0, :, 0, 0])


    # training_data = SplitTrainingDataSet(path, 0)
    # train_dataloader = DataLoader(training_data, batch_size=10, shuffle=False)
    # test_dataloader = DataLoader(validation_data, batch_size = 1, shuffle = False)
##############################################################################################################################
    # ## Define Device, Model, Error Function and Optimizer, and initialize Tensorboard Writer
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model = EEGNet().to(device)

    loss_fn = nn.CrossEntropyLoss()
    # writer = SummaryWriter()
    optimizer = torch.optim.Adam(model.parameters(), lr= 0.0002)

    # Load Saved Model State

    # loadpath = r"C:\Users\Jakob\Desktop\AAU\7Semester\Projekt\P7_projekt\ModelDict3.tar"
    # checkpoint = torch.load(loadpath)
    # model.load_state_dict(checkpoint['model_state_dict'])
    # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    # # lr_sched.load_state_dict(checkpoint['lr'])
    # epoch = checkpoint['epoch']
    # loss = checkpoint['loss']


    #Define Datapath/s
    # paths = [
        # r'C:\Users\Jakob\Desktop\AAU\7Semester\Projekt\P7_projekt\DATA\ON_3\*.mat', 
        # r'C:\Users\Jakob\Desktop\AAU\7Semester\Projekt\P7_projekt\DATA\ON_8\*.mat', 
        # r'C:\Users\Jakob\Desktop\AAU\7Semester\Projekt\P7_projekt\DATA\OFF_3\*.mat',
        # r'C:\Users\Jakob\Desktop\AAU\7Semester\Projekt\P7_projekt\DATA\OFF_8\*.mat'
    # ]

    # # testing_data_path = r"C:\Users\Jakob\Desktop\AAU\7Semester\Projekt\P7_projekt\DATA\Testing"

#############TRAINER FOR EACH TRIAL BY ITSELF --- DOESNT WORK CURRENTLY
    # trn_loss_values = []
    # trn_acc_values = []
    # val_loss_values = []
    # val_acc_values = []
    # acc = 0
    # EPOCHS = 15
    # for i in tqdm.tqdm(range(0, EPOCHS)):
    #     running_loss = 0
    # #     for path in paths:
    #     file1 = r'C:\Users\Jakob\Desktop\AAU\7Semester\Projekt\P7_projekt\DATA\ON_8\ID0002_preprocesseddata_8dBON.mat'
    #     # files = glob.glob(path)
    #     # for file in files:
    #     #     if file == r'C:\Users\Jakob\Desktop\AAU\7Semester\Projekt\P7_projekt\DATA\ON_8\*.mat':
    #     #         maxfileidx = 19
    #     #     else:
    #             # maxfileidx = 20
    #     for e in range(0, 16):
    #         trn_correct = 0
    #         val_correct = 0
    #         trn_acc = 0
    #         val_acc = 0
    #         trn_running_loss = 0
    #         val_running_loss = 0
    #         # if e in range(0,16):
    #         train_data = SplitValData(file1, e)
    #         #     trn=True
    #         # elif e in range(16,20):
    #         #     trn=False
    #         # if trn:
    #         model.train()
    #         dataloader = DataLoader(train_data, batch_size=33, shuffle=True)
    #         # valid_dataloader = DataLoader(valid_data, batch_size=220, shuffle=False)
    #         trn_iteration = iter(dataloader)
    #         trn_data, trn_labels = next(trn_iteration)
    #         for i, x_trn in enumerate(trn_data):
    #             x_trn = torch.unsqueeze(x_trn, 0)
    #             x_trn = torch.unsqueeze(x_trn, 0)
    #             x_trn = x_trn.float().to(device)
    #             y_trn = trn_labels[i].float().to(device)
    #             y_pred = model(x_trn)

    #             loss = loss_fn(y_pred, y_trn)
    #             # writer.add_scalar("Loss/train", loss, e)  
    #             optimizer.zero_grad()
    #             loss.backward()
    #             optimizer.step()
    #             trn_running_loss += loss.item()*x_trn.size(0)
    #             trn_loss_values.append(trn_running_loss)
    #             if torch.argmax(y_pred) == torch.argmax(trn_labels[i]):
    #                 trn_correct +=1
    #             # print(f"\nLABEL: {y_trn} PRED: {y_pred}:")
    #         trn_acc = trn_correct/len(trn_data)
    #         trn_acc_values.append(trn_acc)
    #         # if not trn:
    #         for e in range(16,20):
    #             val_data =  SplitValData(file1, e)
    #             val_iteration = iter(dataloader)
    #             val_data, val_labels = next(val_iteration)
    #             model.eval()
    #             for i, x_val in enumerate(val_data):
    #                 x_val = torch.unsqueeze(x_val, 0)
    #                 x_val = torch.unsqueeze(x_val, 0)
    #                 x_val = x_val.float().to(device)
    #                 y_val = val_labels[i].float().to(device)
    #                 y_pred = model(x_val)

    #                 loss = loss_fn(y_pred, y_val)
    #                 # writer.add_scalar("Loss/train", loss, e)  
    #                 val_running_loss += loss.item()*x_val.size(0)
    #                 val_loss_values.append(val_running_loss)
    #                 if torch.argmax(y_pred) == torch.argmax(val_labels[i]):
    #                     val_correct +=1
    #                 # print(f"\nLABEL: {y} PRED: {y_pred}:") 
    #             val_acc = val_correct/len(val_data)
    #             val_acc_values.append(val_acc)

########TRAINER THAT LOADS ONE ENTIRE FILE --- WORKS
    trn_loss_values = []
    trn_acc_values = []
    val_loss_values = []
    val_acc_values = []
    

    EPOCHS = 30
    for i in tqdm.tqdm(range(0, EPOCHS)):
        trn_correct = 0
        val_correct = 0
        trn_acc = 0
        val_acc = 0
        trn_running_loss = 0
        val_running_loss = 0
        
        file1 = r'C:\Users\Jakob\Desktop\AAU\7Semester\Projekt\P7_projekt\DATA\ON_8\ID0001_preprocesseddata_8dBON.mat'
        file2 = r'C:\Users\Jakob\Desktop\AAU\7Semester\Projekt\P7_projekt\DATA\ON_3\ID0001_preprocesseddata_3dBON.mat'

        model.train()
        training_data = SplitTrainingDataSet(file1)
        valid_data = SplitTrainingDataSet(file2)
        train_dataloader = DataLoader(training_data, batch_size=660, shuffle=True)
        valid_dataloader = DataLoader(valid_data, batch_size=660, shuffle=False)
        trn_data, trn_labels = next(iter(train_dataloader))
        for i, x_trn in enumerate(trn_data):
            x_trn = torch.unsqueeze(x_trn, 0)
            x_trn = torch.unsqueeze(x_trn, 0)
            x_trn = x_trn.float().to(device)
            y_trn = trn_labels[i].float().to(device)
            y_pred = model(x_trn)

            loss = loss_fn(y_pred, y_trn)
            # writer.add_scalar("Loss/train", loss, e)  
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            trn_running_loss += loss.item()
            trn_loss_values.append(trn_running_loss)
            if torch.argmax(y_pred) == torch.argmax(trn_labels[i]):
                trn_correct +=1
            # print(f"\nLABEL: {y} PRED: {y_pred}:") 
        trn_acc = trn_correct/len(trn_data)
        trn_acc_values.append(trn_acc)
        val_data, val_labels = next(iter(valid_dataloader))
        model.eval()
        for i, x_val in enumerate(val_data):
            with torch.no_grad():
                x_val = torch.unsqueeze(x_val, 0)
                x_val = torch.unsqueeze(x_val, 0)
                x_val = x_val.float().to(device)
                y_val = val_labels[i].float().to(device)
                y_pred = model(x_val)

                loss = loss_fn(y_pred, y_val)
                # writer.add_scalar("Loss/train", loss, e)  
                val_running_loss += loss.item()
                val_loss_values.append(val_running_loss)
                if torch.argmax(y_pred) == torch.argmax(val_labels[i]):
                    val_correct +=1
                # print(f"\nLABEL: {y} PRED: {y_pred}:") 
        val_acc = val_correct/len(val_data)
        val_acc_values.append(val_acc)
        

    fig, (ax1, ax2) = plt.subplots(1, 2)
    # if base_save_path:    
    #     st = fig.suptitle(base_save_path, fontsize="x-large")
    
    ax1.title.set_text("Acc")
    ax1.set_xlabel("Epochs")
    l1 = ax1.plot(trn_acc_values, color="red", label='trn_acc')
    l2 = ax1.plot(val_acc_values, color="blue", label='val_acc')
    
    ax2.title.set_text("Loss")
    l3 = ax2.plot(trn_loss_values, color="red", label='trn_loss')
    l4 = ax2.plot(val_loss_values, color="blue", label='val_loss')

    ax1.legend(loc="upper right")
    ax2.legend(loc="upper right")   
    plt.show()
    # # ### Saving the Model
    # torch.save({
    #             'model_state_dict': model.state_dict(),
    #             'optimizer_state_dict': optimizer.state_dict(),
    #             'loss': loss,
    #             # 'lr': lr_sched.state_dict()
    #         }, r"C:\Users\Jakob\Desktop\AAU\7Semester\Projekt\P7_projekt\ModelDict3.tar")
    

if __name__ == '__main__':
    main()